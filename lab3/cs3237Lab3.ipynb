{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS3237 Lab 3 Introduction to Deep Learning\n",
    "\n",
    "<br>\n",
    "<center> Lab Group: B2 </center>\n",
    "\n",
    "| Student Number: | Name:                   |\n",
    "|:----------------|:------------------------|\n",
    "| A0184893L       | Ng Wei Jie, Brandon              |\n",
    "| A7654321Z       | Tan Chin Koo            |\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "We will achieve the following objectives in this lab:\n",
    "\n",
    "    1. An understanding of the practical limitations of using dense networks in complex tasks\n",
    "    2. Hands-on experience in building a deep learning neural network to solve a relatively complex task.\n",
    "    \n",
    "As this lab is more challenging than the previous labs, please work in teams of two persons. Please use the respective categories in the LumiNUS Forum under the \"Labs\" Heading to find a partner within your own group.\n",
    "\n",
    "Each step may take a long time to run. You and your partner may want to work out how to do things simultaneously, but please do not miss out on any learning opportunities.\n",
    "\n",
    "\n",
    "## 2. Submission Instructions\n",
    "\n",
    "This lab is to be done in teams of 2, with each team submitting one copy to  to the group's respective submission folder in the Lab 3 Submissions folder. The file should be called \\<AxxxxxxY\\>.ipynb where \\<AxxxxxxY\\> is the student number of the team leader.\n",
    "\n",
    "**Group B1 (Tuesdays 2 pm to 4 pm) should submit by Friday 18 September 11.59 pm, and Group B2 (Fridays 12 pm to 2 pm) should submit by Monday 21 September 11.59 pm.**\n",
    "\n",
    "## 3. Creating a Dense Network for CIFAR-10\n",
    "\n",
    "We will now begin building a neural network for the CIFAR-10 dataset. The CIFAR-10 dataset consists of 50,000 32x32x3 (32x32 pixels, RGB channels) training images and 10,000 testing images (also 32x32x3), divided into the following 10 categories:\n",
    "\n",
    "    1. Airplane\n",
    "    2. Automobile\n",
    "    3. Bird\n",
    "    4. Cat\n",
    "    5. Deer\n",
    "    6. Dog\n",
    "    7. Frog\n",
    "    8. Horse\n",
    "    9. Ship\n",
    "    10. Truck\n",
    "    \n",
    "In the first two parts of this lab we will create a classifier for the CIFAR-10 dataset.\n",
    "\n",
    "### 3.1 Loading the Dataset\n",
    "\n",
    "We begin firstly by creating a Dense neural network for CIFAR-10. The code below shows how we load the CIFAR-10 dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.datasets import *\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.optimizers import *\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "np.random.seed(24601)\n",
    "\n",
    "def plot_history(history):\n",
    "    #  \"Accuracy\"\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # \"Loss\"\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10():\n",
    "    (train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
    "    train_x = train_x.reshape(train_x.shape[0], 3072) # Question 1\n",
    "    test_x = test_x.reshape(test_x.shape[0], 3072) # Question 1\n",
    "    train_x = train_x.astype('float32')\n",
    "    test_x = test_x.astype('float32')\n",
    "    train_x /= 255.0\n",
    "    test_x /= 255.0\n",
    "    ret_train_y = to_categorical(train_y,10)\n",
    "    ret_test_y = to_categorical(test_y, 10)\n",
    "    \n",
    "    return (train_x, ret_train_y), (test_x, ret_test_y)\n",
    "\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = load_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Question 1\n",
    "\n",
    "Explain what the following two  statements do, and where the number \"3072\" came from (2 MARKS):\n",
    "\n",
    "```\n",
    "  train_x = train_x.reshape(train_x.shape[0], 3072) # Question 1\n",
    "  test_x = test_x.reshape(test_x.shape[0], 3072) # Question 1\n",
    "```\n",
    "\n",
    "**ANSWER: The RGB image has dimensions (32, 32, 3). The reshape method flattens the image so that each observation has 32 * 32 * 3 = 3072 number of features.**\n",
    "\n",
    "*FOR GRADER: _______ / 2*\n",
    "\n",
    "### 3.2 Building the MLP Classifier\n",
    "\n",
    "In the code box below, create a new fully connected (dense) multilayer perceptron classifier for the CIFAR-10 dataset. To begin with, create a network with one hidden layer of 1024 neurons, using the SGD optimizer. You should output the training and validation accuracy at every epoch, and train for 50 epochs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Write your code to build an MLP with one hidden layer of 1024 neurons,\n",
    "with an SGD optimizer. Train for 50 epochs, and output the training and\n",
    "validation accuracy at each epoch.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "input_shape = 3072\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "batch_size = 250\n",
    "verbose = 1\n",
    "\n",
    "def evaluate_model_lr(lr):\n",
    "    # Create the neural network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_shape = (input_shape, ), activation = 'relu'))\n",
    "    model.add(Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "    # Create our optimizer\n",
    "    sgd = SGD(lr = lr)\n",
    "\n",
    "    # 'Compile' the network to associate it with a loss function,\n",
    "    # an optimizer, and what metrics we want to track\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=sgd, \n",
    "        metrics = 'accuracy'\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_x, train_y, \n",
    "        shuffle = True, \n",
    "        epochs = epochs, \n",
    "        validation_data=(test_x, test_y), \n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    \n",
    "    #  \"Accuracy\"\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # \"Loss\"\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Learning Rate: \", lr)\n",
    "    print(\"Training Accuracy: \", history.history['accuracy'][-1])\n",
    "    print(\"Validation Accuracy: \", history.history['val_accuracy'][-1])\n",
    "    \n",
    "lrs = [0.001, 0.01, 0.1]\n",
    "for lr in lrs:\n",
    "    evaluate_model_lr(lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "input_shape = 3072\n",
    "num_classes = 10\n",
    "epochs = 50 \n",
    "batch_size = 250\n",
    "verbose = 1\n",
    "\n",
    "def evaluate_model_m(m):\n",
    "    # Create the neural network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_shape = (input_shape, ), activation = 'relu'))\n",
    "    model.add(Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "    # Create our optimizer\n",
    "    sgd = SGD(lr = 0.01, momentum=m)\n",
    "\n",
    "    # 'Compile' the network to associate it with a loss function,\n",
    "    # an optimizer, and what metrics we want to track\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=sgd, \n",
    "        metrics = 'accuracy'\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_x, train_y, \n",
    "        shuffle = True, \n",
    "        epochs = epochs, \n",
    "        validation_data=(test_x, test_y), \n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    \n",
    "    #  \"Accuracy\"\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # \"Loss\"\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Momentum: \", m)\n",
    "    print(\"Training Accuracy: \", history.history['accuracy'][-1])\n",
    "    print(\"Validation Accuracy: \", history.history['val_accuracy'][-1])\n",
    "    \n",
    "ms = [0.001, 0.01, 0.1, 1.0]\n",
    "for m in ms:\n",
    "    evaluate_model_m(m=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "input_shape = 3072\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "batch_size = 250\n",
    "verbose = 1\n",
    "\n",
    "def evaluate_model_d(d):\n",
    "    # Create the neural network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_shape = (input_shape, ), activation = 'relu'))\n",
    "    model.add(Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "    # Create our optimizer\n",
    "    sgd = SGD(lr = 0.001, momentum=0.01, decay=d)\n",
    "\n",
    "    # 'Compile' the network to associate it with a loss function,\n",
    "    # an optimizer, and what metrics we want to track\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=sgd, \n",
    "        metrics = 'accuracy'\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_x, train_y, \n",
    "        shuffle = True, \n",
    "        epochs = epochs, \n",
    "        validation_data=(test_x, test_y), \n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    \n",
    "    #  \"Accuracy\"\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # \"Loss\"\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Decay: \", d)\n",
    "    print(\"Training Accuracy: \", history.history['accuracy'][-1])\n",
    "    print(\"Validation Accuracy: \", history.history['val_accuracy'][-1])\n",
    "    \n",
    "ds = [0.001, 0.01, 0.1]\n",
    "for d in ds:\n",
    "    evaluate_model_d(d=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr = 0.1, momentum=0.1, decay=0)\n",
    "\n",
    "input_shape = 3072\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "validation_split = 0.2\n",
    "batch_size = 250\n",
    "verbose = 1\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_shape=(input_shape,), activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(\n",
    "    train_x, train_y, \n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    validation_split=validation_split\n",
    ")\n",
    "\n",
    "print(history.history.keys())\n",
    "print(history.history['accuracy'][-1])\n",
    "print(history.history['val_accuracy'][-1])\n",
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_y = model.predict(test_x)\n",
    "accuracy = accuracy_score(test_y, pred_y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Complete the following table on the design choices for your MLP \n",
    "(3 MARKS):\n",
    "\n",
    "| Hyperparameter       | What I used | Why?                  |\n",
    "|:---------------------|:------------|:----------------------|\n",
    "| Optimizer            | SGD         | Specified in question               |\n",
    "| # of hidden layers   | 1           | Specified in question               |\n",
    "| # of hidden neurons  | 1024        | Specified in question               |\n",
    "| Hid layer activation | relu        | Fast, convenient and nonlinear                    |\n",
    "| # of output neurons  | 10          | Dataset has 10 classes              |\n",
    "| Output activation    | softmax     | Obtained probability in each class  |\n",
    "| lr                   | 0.1         | Tested on a range of lrs to find 0.1 the best     |\n",
    "| momentum             | 1           | Tested on a range of momentum to find 1 the best  |\n",
    "| decay                | 0.1         | Test on a range of decay to find 0.1 the best     |\n",
    "| loss                 | categorical_crossentropy   | Model is a multiclass one          |\n",
    "\n",
    "*For TA: ___ / 3* <br>\n",
    "*Code:  ____/ 5* <br>\n",
    "**TOTAL: ____ / 8** <br>\n",
    "\n",
    "#### Question 3:\n",
    "\n",
    "What was your final training accuracy? Validation accuracy? Is there overfitting / underfitting? Explain your answer (5 MARKS)\n",
    "\n",
    "***PLACE YOUR ANSWER HERE ***\n",
    "\n",
    "*FOR GRADER: ______ / 5*\n",
    "\n",
    "### 3.3 Experimenting with the MLP\n",
    "\n",
    "Cut and paste your code from Section 3.2 to the box below (you may need to rename your MLP). Experiment with the number of hidden layers, the number of neurons in each hidden layer, the optimization algorithm, etc. See [Keras Optimizers](https://keras.io/optimizers) for the types of optimizers and their parameters. **Train for 100 epochs.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cut and paste your code from Section 3.2 below, then modify it to get\n",
    "much better results than what you had earlier. E.g. increase the number of\n",
    "nodes in the hidden layer, increase the number of hidden layers,\n",
    "change the optimizer, etc. \n",
    "\n",
    "Train for 100 epochs.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sgd = SGD(lr = 0.1, momentum=0.1)\n",
    "\n",
    "input_shape = 3072\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "validation_split = 0.2\n",
    "batch_size = 250\n",
    "verbose = 1\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_shape=(input_shape,), activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(\n",
    "    train_x, train_y, \n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    validation_split=validation_split\n",
    ")\n",
    "\n",
    "print(history.history.keys())\n",
    "print(history.history['accuracy'][-1])\n",
    "print(history.history['val_accuracy'][-1])\n",
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(test_x)\n",
    "accuracy = accuracy_score(test_y, pred_y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Question 4:\n",
    "\n",
    "Complete the following table with your final design (you may add more rows for the # neurons (layer1) etc. to detail how many neurons you have in each hidden layer). Likewise you may replace the lr, momentum etc rows with parameters more appropriate to the optimizer that you have chosen. (3 MARKS)\n",
    "\n",
    "\n",
    "| Hyperparameter       | What I used | Why?                  |\n",
    "|:---------------------|:------------|:----------------------|\n",
    "| Optimizer            |             |                       |\n",
    "| # of hidden layers   |             |                       |\n",
    "| # neurons(layer1)    |             |                       |\n",
    "| Hid layer1 activation|             |                       |\n",
    "| # neurons(layer2)    |             |                       |\n",
    "| Hid layer2 activation|             |                       |\n",
    "| # of output neurons  |             |                       |\n",
    "| Output activation    |             |                       |\n",
    "| lr                   |             |                       |\n",
    "| momentum             |             |                       |\n",
    "| decay                |             |                       |\n",
    "| loss                 |             |                       |\n",
    "\n",
    "*FOR GRADER: _____ / 3 * <br>\n",
    "*CODE: ______ / 5 *<br>\n",
    "\n",
    "***TOTAL: ______ / 8***\n",
    "\n",
    "#### Question 5\n",
    "\n",
    "What is the final training and validation accuracy that you obtained after 150 epochs. Is there considerable improvement over Section 3.2? Are there still signs of underfitting or overfitting? Explain your answer (5 MARKS)\n",
    "\n",
    "***Write your answers here***\n",
    "\n",
    "*FOR GRADER: ______ / 5 *\n",
    "\n",
    "#### Question 6\n",
    "\n",
    "Write a short reflection on the practical difficulties of using a dense MLP to classsify images in the CIFAR-10 datasets. (3 MARKS)\n",
    "\n",
    "***Write your answers here***\n",
    "\n",
    "*FOR GRADER: _______ /3*\n",
    "\n",
    "----\n",
    "\n",
    "## 4. Creating a CNN for the MNIST Data Set\n",
    "\n",
    "In this section we will now create a convolutional neural network (CNN) to classify images in the MNIST dataset that we used in the previous lab. Let's go through each part to see how to do this.\n",
    "\n",
    "### 4.1 Loading the MNIST Dataset\n",
    "\n",
    "As always we will load the MNIST dataset, scale the inputs to between 0 and 1, and convert the Y labels to one-hot vectors. However unlike before we will not flatten the 28x28 image to a 784 element vector, since CNNs can inherently handle 2D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    (train_x, train_y),(test_x, test_y) = mnist.load_data()\n",
    "    train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)\n",
    "    test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)\n",
    "\n",
    "    train_x=train_x.astype('float32')\n",
    "    test_x = test_x.astype('float32')\n",
    "    \n",
    "    train_x /= 255.0\n",
    "    test_x /= 255.0\n",
    "        \n",
    "    train_y = to_categorical(train_y, 10)\n",
    "    test_y = to_categorical(test_y, 10)\n",
    "        \n",
    "    return (train_x, train_y), (test_x, test_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Building the CNN\n",
    "\n",
    "We will now build the CNN. Unlike before we will create a function to produce the CNN. We will also look at how to save and load Keras models using \"checkpoints\", particularly \"ModelCheckpoint\" that saves the model each epoch.\n",
    "\n",
    "Let's begin by creating the model. We call os.path.exists to see if a model file exists, and call \"load_model\" if it does. Otherwise we create a new model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model loads a model from a hd5 file.\n",
    "\n",
    "MODEL_NAME = 'mnist-cnn.hd5'\n",
    "\n",
    "if os.path.exists(MODEL_NAME) and os.path.isdir(MODEL_NAME):\n",
    "    shutil.rmtree(MODEL_NAME)\n",
    "\n",
    "def buildmodel(model_name):\n",
    "    if os.path.exists(model_name):\n",
    "        model = load_model(model_name)                                                                                             \n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(5,5),\n",
    "        activation='relu',\n",
    "        input_shape=(28, 28, 1), padding='same')) # Question 7\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # Question 8\n",
    "        model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
    "        model.add(Conv2D(128, kernel_size=(5,5), activation='relu'))\n",
    "        model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "        model.add(Flatten()) # Question 9\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Question 7\n",
    "\n",
    "The first layer in our CNN is a 2D convolution kernel, shown here:\n",
    "\n",
    "```\n",
    "        model.add(Conv2D(32, kernel_size=(5,5),\n",
    "        activation='relu',\n",
    "        input_shape=(28, 28, 1), padding='same')) # Question 7\n",
    "```\n",
    "\n",
    "Why is the input_shape set to (28, 28, 1)? What does this mean? What does \"padding = 'same'\" mean? (4 MARKS)\n",
    "\n",
    "***The images in minist dataset has dimension (28, 28, 1) and the model is taking the images directly of the shape (28, 28, 1). The dimensions of image is 28 by 28. The third value 1 indicates that the image does not have RGB values and is monotone. The hyperparameter \"same\" results in padding evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.***\n",
    "\n",
    "*FOR GRADER: ______ / 4*\n",
    "\n",
    "#### Question 8\n",
    "\n",
    "The second layer is the MaxPooling2D layer shown below:\n",
    "\n",
    "```\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # Question 8\n",
    "```\n",
    "\n",
    "What other types of pooling layers are available? What does 'strides = 2' mean? (3 MARKS)\n",
    "\n",
    "***The list of pooling layers can be found in this link https://keras.io/api/layers/pooling_layers/. The layers applicable for 2d are MaxPooling2D, AveragePooling2D, GlobalMaxPooling2D, and GlobalAveragePooling2D. The hyperparameter 'strides = 2' specifies that the pooling window moves 2 units for each pooling step.***\n",
    "\n",
    "*FOR GRADER: _____ / 3*\n",
    "\n",
    "\n",
    "#### Question 9\n",
    "\n",
    "What does the \"Flatten\" layer here do? Why is it needed?\n",
    "\n",
    "```\n",
    "        model.add(Flatten()) # Question 9\n",
    "```\n",
    "\n",
    "***Flattening converts the data into a 1-dimensional array for inputting it to the next layer. This layer is necessary to reshape the output of the convolutional layers, flattens the data to create a single long feature vector to be used by the final dense layer for classification.***\n",
    "\n",
    "*FOR GRADER: ____ / 2*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "### 4.3 Training the CNN\n",
    "\n",
    "Let's now train the CNN. In this example we introduce the idea of a \"callback\", which is a routine that Keras calls at the end of each epoch. Specifically we look at two callbacks:\n",
    "\n",
    "    1. ModelCheckpoint: When called, Keras saves the model to the specified filename.\n",
    "    \n",
    "    2. EarlyStopping: When called, Keras checks if it should stop the training prematurely.\n",
    "    \n",
    "\n",
    "Let's look at the code to see how training is done, and how callbacks are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_x, train_y, epochs, test_x, test_y, model_name):\n",
    "\n",
    "    model.compile(optimizer=SGD(lr=0.01, momentum=0.7), \n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    savemodel = ModelCheckpoint(model_name)\n",
    "    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n",
    "\n",
    "    print(\"Starting training.\")\n",
    "\n",
    "    model.fit(x=train_x, y=train_y, batch_size=32,\n",
    "    validation_data=(test_x, test_y), shuffle=True,\n",
    "    epochs=epochs, \n",
    "    callbacks=[savemodel, stopmodel])\n",
    "\n",
    "    print(\"Done. Now evaluating.\")\n",
    "    loss, acc = model.evaluate(x=test_x, y=test_y)\n",
    "    print(\"Test accuracy: %3.2f, loss: %3.2f\"%(acc, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there isn't very much that is unusual going on; we compile the model with our loss function and optimizer, then call fit, and finally evaluate to look at the final accuracy for the test set.  The only thing unusual is the \"callbacks\" parameter here in the fit function call\n",
    "\n",
    "```\n",
    "    model.fit(x=train_x, y=train_y, batch_size=32,\n",
    "    validation_data=(test_x, test_y), shuffle=True,\n",
    "    epochs=epochs, \n",
    "    callbacks=[savemodel, stopmodel])\n",
    "```\n",
    "\n",
    "----\n",
    "\n",
    "#### Question 10.\n",
    "\n",
    "What does do the min_delta and patience parameters do in the EarlyStopping callback, as shown below? (2 MARKS)\n",
    "\n",
    "***The paramter min_delta is the minimum change in the monitored quantity to qualify as an improvement. For example, if the absolute change in the accuracy is less than min_delta then the model will count as no improvement. The parameter patience is the number of epochs with no improvement after which training will be stopped.***\n",
    "\n",
    "```\n",
    "    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Putting it together.\n",
    "\n",
    "Now let's run the code and see how it goes (Note: To save time we are training for only 5 epochs; we should train much longer to get much better results):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y),(test_x, test_y) = load_mnist()\n",
    "model = buildmodel(MODEL_NAME)\n",
    "train(model, train_x, train_y, 5, test_x, test_y, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----\n",
    "\n",
    "#### Question 11.\n",
    "\n",
    "Compare the relative advantages and disadvantages of CNN vs. the Dense MLP that you build in sections 3.2 and 3.3. What makes CNNs better (or worse)? (3 MARKS)\n",
    "\n",
    "***MLP is used to be applied in computer vision but is now succeded by CNN. The disadvantage of MLP is that MLP takes in flatten inputs of images and the spatial information of these images are lost. Another disadvantage of MLP is that MLP has fully connected layers and the paramters grows quickly. Training MLP is difficult as overfitting tends to occur and the model lose the ability to generalize. The advantage of MLP is that a light weight MLP can easily achieve high accuracy for dataset such as MNIST one. Another advantage of MLP is that MLP can be used as a baseline point of comparison to confirm that other models appear to perform better. The advantage of CNN is that the spatial information of input images is not lost as the images are input into the model without reshaping and flattening. Another advantage of CNN is that CNN can account for local connectivity because each filter is panned around the entire image according to certain size and stride, allowing the filter to find and match patterns no matter where the pattern is located in a given image.***\n",
    "\n",
    "*FOR TA: ______ / 3*\n",
    "\n",
    "## 5. Making a CNN for the CIFAR-10 Dataset\n",
    "\n",
    "Now comes the fun part: Using the example above for creating a CNN for the MNIST dataset, now create a CNN in the box below for the MNIST-10 dataset. At the end of each epoch save the model to a file called \"cifar.hd5\" (note: the .hd5 is added automatically for you).\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 12.\n",
    "\n",
    "Summarize your design in the table below (the actual coding cell comes after this):\n",
    "\n",
    "| Hyperparameter       | What I used | Why?                  |\n",
    "|:---------------------|:------------|:----------------------|\n",
    "| Optimizer            | Adam | Adam converges faster. Adam computes adaptive learning rates for each parameter that is tuned so less tuning to the learning rate are needed during training. |\n",
    "| Input shape          | (32, 32, 3) | Images in the dataset have size 32 pixels by 32 pixels and RGB values per pixel. |\n",
    "| First layer          | Conv2D | Conv2D layer is used to retain the spatial information of images as inputs to the model. |\n",
    "| Second layer         | BatchNormalization | BatchNormalization is used to standardized outputs of previous layers. BatchNormalization can help to accelerate the training process of a neural network and can improve the performance of the model via a modest regularization effect. |\n",
    "| Add more layers      | Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Dense | Layers of Conv2D and BatchNormalization are to create a deeper model to help the model learn the complex features. MaxPooling2D are down sampled or pooled feature maps that highlight the most present feature in the patch. Dropout is used to reduce overfitting. Flatten is to reshape the output of the convolutional layer for classification. Dense layers are added to help with classification and learn outputs from previous layers. |\n",
    "| if needed            | Activation, Kernel Initializer, Kernel Regularizer | ELU is produce activations instead of letting them be zero when calculating the gradient, avoids the dead relu problem and produces negative outputs, which helps the network nudge weights and biases in the right directions. L2 regularizers reduce chances of overfitting. |\n",
    "| Dense layer | Dense(256) and Dense(10) | The last layer uses Dense(10) to classify images to 10 categories and Dense(256) as second last layers to learn outputs from previous layer that gives better testing accuracy. |\n",
    "\n",
    "\n",
    "*FOR TA:*\n",
    "*Table: ________ / 3* <br>\n",
    "*Code: _________/ 7* <br>\n",
    "**TOTAL: _______ / 10** <br>\n",
    "\n",
    "---\n",
    "\n",
    "***TOTAL: _______ / 55***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write your code for your CNN for the CIFAR-10 dataset here. \n",
    "\n",
    "Note: train_x, train_y, test_x, test_y were changed when we called \n",
    "load_mnist in the previous section. You will now need to call load_cifar10\n",
    "again.\n",
    "\n",
    "\"\"\"\n",
    "MODEL_NAME = 'cifar.hd5'\n",
    "\n",
    "if os.path.exists(MODEL_NAME) and os.path.isdir(MODEL_NAME):\n",
    "    shutil.rmtree(MODEL_NAME)\n",
    "\n",
    "def buildmodel(model_name, lr = 0.001, dc = 1e-5, dr = 0.5):\n",
    "    if os.path.exists(model_name):\n",
    "        model = load_model(model_name)                                                                                             \n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(1e-4), padding='same', input_shape=(32, 32, 3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(1e-4), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(1e-4), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(1e-4), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(1e-4), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(1e-4), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='elu', kernel_initializer='he_uniform'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dr))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        \n",
    "        opt = Adam(lr = lr, decay = dc)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_x, train_y, epochs, test_x, test_y, model_name):\n",
    "    savemodel = ModelCheckpoint(model_name)\n",
    "    stopmodel = EarlyStopping(min_delta=0.001, patience=10)\n",
    "    \n",
    "    print(\"Starting training.\")\n",
    "    history = model.fit(x=train_x, y=train_y, batch_size=32, validation_data=(test_x, test_y), shuffle=True, epochs=epochs, callbacks=[savemodel, stopmodel])\n",
    "    plot_history(history)\n",
    "    print(\"Done. Now evaluating.\")\n",
    "    \n",
    "    loss, acc = model.evaluate(x=test_x, y=test_y)\n",
    "    print(\"Test accuracy: %3.2f, loss: %3.2f\"%(acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10():\n",
    "    (train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
    "    train_x = train_x.reshape(train_x.shape[0], 32, 32, 3)\n",
    "    test_x = test_x.reshape(test_x.shape[0], 32, 32, 3)\n",
    "    train_x = train_x.astype('float32')\n",
    "    test_x = test_x.astype('float32')\n",
    "    train_x /= 255.0\n",
    "    test_x /= 255.0\n",
    "    ret_train_y = to_categorical(train_y,10)\n",
    "    ret_test_y = to_categorical(test_y, 10)\n",
    "    \n",
    "    return (train_x, ret_train_y), (test_x, ret_test_y)\n",
    "\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = load_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 1.6161 - accuracy: 0.4791WARNING:tensorflow:From /home/nwjbrandon/env/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: cifar.hd5/assets\n",
      "1563/1563 [==============================] - 197s 126ms/step - loss: 1.6161 - accuracy: 0.4791 - val_loss: 1.1181 - val_accuracy: 0.6547\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 1.1012 - accuracy: 0.6485INFO:tensorflow:Assets written to: cifar.hd5/assets\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 1.1012 - accuracy: 0.6485 - val_loss: 0.9213 - val_accuracy: 0.7148\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.9620 - accuracy: 0.7052INFO:tensorflow:Assets written to: cifar.hd5/assets\n",
      "1563/1563 [==============================] - 222s 142ms/step - loss: 0.9620 - accuracy: 0.7052 - val_loss: 0.8833 - val_accuracy: 0.7337\n",
      "Epoch 4/100\n",
      "1397/1563 [=========================>....] - ETA: 21s - loss: 0.8819 - accuracy: 0.7362"
     ]
    }
   ],
   "source": [
    "(train_x, train_y), (test_x, test_y) = load_cifar10()\n",
    "model = buildmodel(MODEL_NAME)\n",
    "train(model, train_x, train_y, 100, test_x, test_y, MODEL_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
